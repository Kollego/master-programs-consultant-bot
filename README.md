# ITMO Masters Program Chat Bot

Этот проект реализует чат-бота, который:
- парсит учебные планы двух магистратур ИТМО: `ИИ` и `AI Product` (прямо с сайта, с уважением к robots.txt);
- извлекает ключевые факты (стоимость, язык, срок обучения, факультет);
- отвечает на вопросы по содержимому программ в диалоговом режиме (Telegram-бот);
- рекомендует выборные дисциплины с учётом бэкграунда абитуриента.

## Описание решения
- Решение разрабатывалось в редакторе Cursor с использованием **GPT-5** как парного программиста.
- Архитектура: скрейпинг страниц → подготовка корпуса → эмбеддинги (SentenceTransformers `all-MiniLM-L6-v2`) → FAISS-индекс → Telegram-бот (`python-telegram-bot`).
- Используется подход **RAG без удалённых LLM-запросов**: на вопрос ищутся релевантные фрагменты из индекса, затем ответ формируется на их основе с лёгкими эвристиками (проверка релевантности, обработка вопросов о стоимости/преподавателях).
- Запустить локальную LLM получилось, но результат её работы оставляет желать лучшего. Скорее всего, для более корректной работы следует инегрировать API.

## Быстрый старт



## Запуск одной командой

Все шаги автоматизированы в скрипте. Он создаст venv, установит зависимости, при отсутствии — создаст `.env`, спарсит сайт, соберёт корпус и индекс, а затем запустит бота, если задан токен.

```bash
bash scripts/bootstrap.sh
```

Если токен не задан, скрипт всё подготовит и выведет подсказку, как его добавить и запустить бота вручную.

### Где хранить токен

- Файл `.env` должен лежать в корне репозитория.
- Укажите один из переменных: `TELEGRAM_BOT_TOKEN` или `TELEGRAM_TOKEN`.
- Пример:

```env
TELEGRAM_BOT_TOKEN=123456:ABCDEF...
```

### Локальная LLM (по умолчанию выключена)

Локальная модель Hugging Face может быть включена для генерации ответа поверх найденных фрагментов (RAG → LLM → ответ). Внешние API не используются.

- Включить можно флагом:

```bash
# Полный конвейер со сборкой индекса и ботом с LLM
bash scripts/bootstrap.sh --llm

# Если запускаете бота вручную с уже собранным индексом:
python -m src.bot.main           # без LLM (по умолчанию)
python -m src.bot.main --no-llm  # эквивалентно
python -m src.bot.main           # без флагов — LLM off
# включить LLM из скрипта: bash scripts/bootstrap.sh --llm
```

- По умолчанию используется компактная чат‑модель `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (когда LLM включена). На CPU медленно; на macOS с MPS заметно быстрее. При любой ошибке локальной генерации бот автоматически вернётся к эвристическому ответу на основе контекста.

## Команды бота
- `/start` — выбор программы и начало диалога
- `/help` — справка
- `/programs` — краткая информация о доступных программах
- `/plan` — ссылка/сводка по учебному плану выбранной программы
- `/recommend` — рекомендации по выборным дисциплинам (бот запросит бэкграунд)

## Архитектура
- `src/scraping/fetch.py` — загрузка HTML с учётом robots.txt и ретраев
- `src/scraping/parse_itmo.py` — парсинг страниц программ ИТМО и сохранение структурированных данных (включая стоимость/язык/срок/факультет)
- `src/pipeline/build_corpus.py` — подготовка корпусных документов для RAG (включая факты)
- `src/pipeline/index.py` — построение векторного индекса (SentenceTransformers + FAISS)
- `src/bot/` — Telegram-бот, RAG-ответы, фильтры релевантности и рекомендации

## Данные
- `data/processed/` — JSON с программами и корпусом
- `data/vector_store/` — индексы FAISS

## Тройка команд для полного цикла
```bash
python -m src.scraping.parse_itmo --urls https://abit.itmo.ru/program/master/ai https://abit.itmo.ru/program/master/ai_product --out data/processed/programs.json
python -m src.pipeline.build_corpus --in data/processed/programs.json --out data/processed/corpus.jsonl
python -m src.pipeline.index --in data/processed/corpus.jsonl --index_dir data/vector_store
```

